# LangChain + vLLM 依赖管理问答系统

这是一个基于LangChain和vLLM的依赖管理专业问答系统，可以回答与软件依赖管理相关的各种问题。

## 系统功能

- 回答关于软件依赖管理的专业问题
- 支持主流依赖管理工具（pip、conda、npm、maven等）的相关咨询
- 提供依赖冲突解决、虚拟环境、依赖锁定等最佳实践建议
- 支持CI/CD和容器化环境中的依赖管理策略咨询
- 具有问题相关性检查功能，自动识别不相关问题
- 包含缓存机制，提高重复问题的响应速度
- 完整的错误处理和日志系统

## 目录结构

```
langchain/vllm/
├── vllm_dependency_qa.py    # 主程序文件
├── vllm_config.py           # 配置文件
├── test_vllm_dependency_qa.py  # 系统功能测试脚本
├── test_direct_vllm.py      # 直接模型测试脚本
├── README.md                # 说明文档
```

## 前置条件

1. 安装Python 3.8或更高版本
2. 安装必要的依赖库
3. 安装vLLM并配置本地模型（无需启动vLLM服务）

## 安装依赖

```bash
pip install langchain langchain_community vllm numpy pandas scikit-learn faiss-cpu requests
```

## 准备本地模型

系统会自动直接加载本地模型，无需启动vLLM服务。请确保：

1. 模型文件已完整下载到本地
2. 在配置文件中正确指定了模型路径
3. 系统有足够的内存/GPU内存来加载模型

有关模型路径配置的详细信息，请参见下方的配置说明部分。

## 配置说明

系统的主要配置参数位于`vllm_config.py`文件中，可以根据需要进行修改：

### 1. 模型配置

```python
MODEL_CONFIG = {
    "model": "E:\\AI\\models\\vllm\\gpt2",  # 本地模型路径
    "temperature": 0.1,  # 低温度使输出更确定
    "max_tokens": 1024,  # 最大生成令牌数
    "top_p": 0.95,  # 采样参数
    "vllm_kwargs": {
        "gpu_memory_utilization": 0.8,  # GPU内存使用率
        "max_model_len": 4096,  # 最大模型长度
        "tensor_parallel_size": 1,  # 张量并行大小
        "trust_remote_code": True,  # 信任远程代码
        "dtype": "auto",  # 自动选择数据类型
        "disable_log_requests": True,  # 禁用请求日志
        "disable_log_stats": False,  # 启用统计日志
    }
}
```

关键配置参数说明：
- `model`: 本地模型的完整路径，确保系统能够找到模型文件
- 其他参数可以根据您的硬件配置和需求进行调整

### 2. 检索配置

调整检索相关参数，如返回文档数量

### 3. 知识库配置

指定知识库文件路径和相关设置

### 4. 日志配置

设置日志级别和输出格式

### 5. 缓存配置

调整缓存大小和过期时间

## 使用方法

### 1. 基本使用

```python
from vllm_dependency_qa import VLLMDependencyQA

# 创建问答系统实例
qa_system = VLLMDependencyQA()

# 提问
question = "什么是依赖管理？"
response = qa_system.ask_question(question)

print(response)
```

### 2. 运行主程序

直接运行主程序可以启动交互式问答界面：

```bash
python vllm_dependency_qa.py
```

这将启动一个交互式会话，系统会自动加载本地模型，您可以直接输入问题并获取回答。

### 3. 运行测试脚本

有两个测试脚本可供使用：

#### 3.1 直接模型测试

```bash
python test_direct_vllm.py
```

此脚本专门用于测试直接加载本地模型的功能，包括模型路径检查、模型加载和推理测试。它会输出模型加载时间、推理时间和推理结果。

#### 3.2 系统功能测试

```bash
python test_vllm_dependency_qa.py
```

此脚本会运行一系列与依赖管理相关的问题，并显示每个问题的响应时间和部分回答内容。

## 示例问题

```
# 基础概念
什么是依赖管理？
为什么依赖管理很重要？

# 工具比较
pip和conda有什么区别？
npm和yarn哪个更好？

# 问题解决
如何解决依赖冲突问题？
如何查看项目的依赖树？

# 最佳实践
什么是虚拟环境，为什么需要它？
如何在生产环境中锁定依赖版本？

# CI/CD与容器化
如何在CI/CD流程中管理依赖？
Docker容器中如何优化依赖安装？
```

## 性能优化建议

1. **使用合适的模型**：根据需求选择合适大小的模型，平衡性能和准确性
2. **调整检索参数**：适当减少`search_kwargs`中的`k`值可以提高响应速度
3. **启用缓存**：系统已内置缓存机制，重复问题会更快响应
4. **优化硬件资源**：如有可能，使用GPU加速推理过程
5. **调整vLLM配置**：可以根据vLLM文档调整各种参数以获得最佳性能

## 故障排除

1. **模型加载失败**：
   - 确保vLLM正确安装: `pip install vllm`
   - 检查模型路径是否正确且包含完整的模型文件
   - 对于自定义模型，确保设置了`trust_remote_code=True`
   - 查看错误日志，确认是否有特定的错误信息

2. **知识库加载失败**：检查知识库文件是否存在且格式正确

3. **响应时间过长**：考虑使用更小的模型或增加硬件资源

4. **内存不足**：
   - 调整vLLM的内存相关配置参数
   - 尝试使用`--quantization`参数进行模型量化
   - 关闭其他占用内存的程序

## 扩展指南

1. **添加新的知识库**：可以在`data`目录下添加新的知识库文件，并在配置中指定
2. **自定义相关性检查**：修改配置文件中的`RELEVANCE_KEYWORDS`来调整相关性检查逻辑
3. **集成新模型**：在配置文件中修改`MODEL_NAME`以使用不同的模型

## 注意事项

1. 本系统仅专注于软件依赖管理领域的问题
2. 对于不相关的问题，系统会礼貌地拒绝回答
3. 所有响应都基于知识库和模型生成，可能需要人工验证关键信息
4. 在生产环境中使用前，建议进行充分的测试和验证
5. 直接加载本地模型可能需要较多的内存和计算资源
6. 首次加载模型可能需要较长时间，请耐心等待
7. 不同模型的性能和输出质量可能有所不同，建议根据需求选择合适的模型